---
title: LangChain V1 Migration Guide
sidebarTitle: Migration guide
---

## Migrating to `create_agent`

### Overview

| Section | What changed |
|---------|--------------|
| [Import path](#import-path) | Package moved from `langgraph.prebuilt` to `langchain.agents` |
| [Prompts](#prompts) | Parameter renamed to `system_prompt`, dynamic prompts use middleware |
| [Pre model hook](#pre-model-hook) | Replaced by middleware with `before_model` method |
| [Post model hook](#post-model-hook) | Replaced by middleware with `after_model` method |
| [Custom state](#custom-state) | Defined in middleware, `TypedDict` only |
| [Model](#model) | Dynamic selection via middleware, pre-bound models not supported |
| [Tools](#tools) | Tool error handling moved to middleware with `wrap_tool_call` |
| [Structured output](#structured-output) | prompted output removed, use `ToolStrategy`/`ProviderStrategy` |
| [Streaming node name](#streaming-node-name-rename) | Node name changed from `"agent"` to `"model"` |
| [Runtime context](#runtime-context) | Dependency injection via `context` argument instead of `config["configurable"]` |

### Import path

The import path for the agent prebuilt has changed from `langgraph.prebuilt` to `langchain.agents`.
The name of the function has changed from `create_react_agent` to `create_agent`.

<CodeGroup>
```python V1 (new)
from langchain.agents import create_agent
```
```python V0 (old)
from langgraph.prebuilt import create_react_agent
```
</CodeGroup>

### Prompts

#### Static prompt rename

The `prompt` parameter has been renamed to `system_prompt`.

<CodeGroup>
```python V1 (new)
from langchain.agents import create_agent

agent = create_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=[check_weather],
    system_prompt="You are a helpful assistant"  # [!code highlight]
)
```
```python V0 (old)
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=[check_weather],
    prompt="You are a helpful assistant"  # [!code highlight]
)
```
</CodeGroup>

#### `SystemMessage` to string

If using `SystemMessage` objects in the system prompt, extract the string content:

<CodeGroup>
```python V1 (new)
from langchain.agents import create_agent

agent = create_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=[check_weather],
    system_prompt="You are a helpful assistant"  # [!code highlight]
)
```
```python V0 (old)
from langchain_core.messages import SystemMessage
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=[check_weather],
    prompt=SystemMessage(content="You are a helpful assistant")  # [!code highlight]
)
```
</CodeGroup>

#### Dynamic prompts

Dynamic prompts are a core context engineering patternâ€”adapting what you tell the model based on the current conversation state. The `@dynamic_prompt` decorator makes this simple:

<CodeGroup>
```python V1 (new)
from dataclasses import dataclass

from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest
from langgraph.runtime import Runtime

@dataclass
class Context:  # [!code highlight]
    user_role: str = "user"

@dynamic_prompt  # [!code highlight]
def dynamic_prompt(request: ModelRequest) -> str:  # [!code highlight]
    user_role = request.runtime.context.user_role
    base_prompt = "You are a helpful assistant."

    if user_role == "expert":
        prompt = (
            f"{base_prompt} Provide detailed technical responses."
        )
    elif user_role == "beginner":
        prompt = (
            f"{base_prompt} Explain concepts simply and avoid jargon."
        )
    else:
        prompt = base_prompt

    return prompt  # [!code highlight]

agent = create_agent(
    model="openai:gpt-4o",
    tools=tools,
    middleware=[dynamic_prompt],  # [!code highlight]
    context_schema=Context
)

# Use with context
agent.invoke(
    {"messages": [{"role": "user", "content": "Explain async programming"}]},
    context=Context(user_role="expert")
)
```

```python V0 (old)
from dataclasses import dataclass

from langgraph.prebuilt import create_react_agent, AgentState
from langgraph.runtime import get_runtime

@dataclass
class Context:
    user_role: str

def dynamic_prompt(state: AgentState) -> str:
    runtime = get_runtime(Context)  # [!code highlight]
    user_role = runtime.context.user_role
    base_prompt = "You are a helpful assistant."

    if user_role == "expert":
        return f"{base_prompt} Provide detailed technical responses."
    elif user_role == "beginner":
        return f"{base_prompt} Explain concepts simply and avoid jargon."
    return base_prompt

agent = create_react_agent(
    model="openai:gpt-4o",
    tools=tools,
    prompt=dynamic_prompt,
    context_schema=Context
)

# Use with context
agent.invoke(
    {"messages": [{"role": "user", "content": "Explain async programming"}]},
    context=Context(user_role="expert")
)
```
</CodeGroup>


### Pre model hook

Pre-model hooks are now implemented as middleware with the `before_model` method.
This new pattern is more extensible - you can define multiple middlewares to run before the model is called,
reusing common patterns across different agents.

Common use cases here are:
* Summarizing conversation history
* Trimming messages
* Input guardrails, like PII redaction

V1 now has summarization middleware built in:

<CodeGroup>
```python V1 (new)
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=tools,
    middleware=[
        SummarizationMiddleware(  # [!code highlight]
            model="anthropic:claude-4-5-sonnet",  # [!code highlight]
            max_tokens_before_summary=1000  # [!code highlight]
        )  # [!code highlight]
    ]  # [!code highlight]
)
```
```python V0 (old)
from langgraph.prebuilt import create_react_agent, AgentState

def custom_summarization_function(state: AgentState):
    """Custom logic for message summarization."""
    ...

agent = create_react_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=tools,
    pre_model_hook=custom_summarization_function
)
```
</CodeGroup>

### Post model hook

Post-model hooks are now implemented as middleware with the `after_model` method.
This new pattern is more extensible - you can define multiple middlewares to run after the model is called,
reusing common patterns across different agents.

Common use cases here are:
* Human in the loop
* Output guardrails

V1 has a built in middleware for human in the loop approval for tool calls:

<CodeGroup>
```python V1 (new)
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware

agent = create_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=[read_email, send_email],
    middleware=[HumanInTheLoopMiddleware(
        interrupt_on={
            "send_email": True,
            "description": "Please review this email before sending"
        },
    )]
)
```

```python V0 (old)
from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt import AgentState

def custom_human_in_the_loop_hook(state: AgentState):
    """Custom logic for human in the loop approval."""
    ...

agent = create_react_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=[read_email, send_email],
    post_model_hook=custom_human_in_the_loop_hook
)
```
</CodeGroup>

### Custom state

Custom state is now defined in middleware using the `state_schema` attribute:

<CodeGroup>
```python V1 (new)
from typing import Annotated
from langchain_core.tools import tool
from langchain.agents import create_agent  # [!code highlight]
from langchain.agents.middleware import AgentMiddleware, AgentState  # [!code highlight]
from langgraph.prebuilt import InjectedState

# Define custom state extending AgentState
class CustomState(AgentState):
    user_name: str

# Create middleware that manages custom state
class UserStateMiddleware(AgentMiddleware[CustomState]):  # [!code highlight]
    state_schema = CustomState  # [!code highlight]

@tool  # [!code highlight]
def greet(
    state: Annotated[CustomState, InjectedState]
) -> str:
    """Use this to greet the user by name."""
    user_name = state.get("user_name", "Unknown")  # [!code highlight]
    return f"Hello {user_name}!"

agent = create_agent(  # [!code highlight]
    model="anthropic:claude-4-5-sonnet",
    tools=[greet],
    middleware=[UserStateMiddleware()]  # [!code highlight]
)
```
```python V0 (old)
from typing import Annotated
from langgraph.prebuilt import InjectedState, create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState

class CustomState(AgentState):
    user_name: str

def greet(
    state: Annotated[CustomState, InjectedState]
) -> str:
    """Use this to greet the user by name."""
    user_name = state["user_name"]
    return f"Hello {user_name}!"

agent = create_react_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=[greet],
    state_schema=CustomState
)
```
</CodeGroup>

<Note>
    Custom state is defined by creating a class that extends `AgentState` and assigning it to the middleware's `state_schema` attribute.
</Note>

#### State type restrictions

`create_agent` now only supports `TypedDict` for state schemas. Pydantic models and dataclasses are no longer supported for simplicity and consistency.

<CodeGroup>
```python V1 (new)
from typing import TypedDict  # [!code highlight]

class AgentState(TypedDict):  # [!code highlight]
    messages: list
    user_id: str
    context: dict

agent = create_react_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=tools,
    middleware=[middleware_with_agent_state]
)
```

```python V0 (old)
from pydantic import BaseModel
from langgraph.graph import StateGraph

class AgentState(BaseModel):
    messages: list
    user_id: str
    context: dict

agent = create_react_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=tools,
    state_schema=AgentState
)
```
</CodeGroup>

**Migration steps**:
1. Convert Pydantic models to `TypedDict`
2. Convert dataclasses to `TypedDict`
3. Remove validators and default values (handle these in your node functions instead)

<CodeGroup>
```python V1 (new) - TypedDict
from typing import TypedDict  # [!code highlight]

class AgentState(TypedDict, total=False):  # [!code highlight]
    messages: list
    user_id: str
    retry_count: int

# Handle defaults in your initialization code
def create_initial_state() -> AgentState:  # [!code highlight]
    return {  # [!code highlight]
        "messages": [],  # [!code highlight]
        "user_id": "",  # [!code highlight]
        "retry_count": 0  # [!code highlight]
    }  # [!code highlight]
```
```python V0 (old) - Dataclass
from dataclasses import dataclass, field

@dataclass
class AgentState:
    messages: list = field(default_factory=list)
    user_id: str = ""
    retry_count: int = 0
```
</CodeGroup>

### Model

Dynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). `create_react_agent` released in V0.6 of `langgraph-prebuilt` supported dynamic model and tool selection
via a callable passed to the `model` parameter.

This functionality has been ported to the middleware interface in V1.

#### Dynamic model selection

<CodeGroup>
```python V1 (new)
from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware

basic_model = ChatOpenAI(model="gpt-5-nano")
advanced_model = ChatOpenAI(model="gpt-5")

class DynamicModelMiddleware(AgentMiddleware):

    def wrap_model_call(self, request: ModelRequest, handler: ModelRequestHandler) -> AIMessage:
        model_call = request.model_call
        if len(request.state.messages) > 10:
            model = advanced_model
        else:
            model = basic_model

        return handler(model_call.replace(model=model))

agent = create_agent(
    model=basic_model,
    tools=tools,
    middleware=[DynamicModelMiddleware(select_model)]
)
```
```python V0 (old)
from langgraph.prebuilt import create_react_agent, AgentState
from langchain_openai import ChatOpenAI

basic_model = ChatOpenAI(model="gpt-5-nano")
advanced_model = ChatOpenAI(model="gpt-5")

def select_model(state: AgentState) -> BaseChatModel:
    # use a more advanced model for longer conversations
    if len(state.messages) > 10:
        return advanced_model
    return basic_model

agent = create_react_agent(
    model=select_model,
    tools=tools,
)
```
</CodeGroup>

#### Pre-bound models

To better support structured output, `create_agent` no longer accepts pre-bound models with tools or configuration:

```python
# No longer supported
model_with_tools = ChatOpenAI().bind_tools([some_tool])
agent = create_agent(model_with_tools, tools=[])

# Use instead
agent = create_agent("openai:gpt-4o-mini", tools=[some_tool])
```

<Note>
Dynamic model functions can return pre-bound models if structured output is *not* used.
</Note>

### Tools

The `tools` argument to `create_agent` accepts a list of:

* LangChain `BaseTool` instances (functions decorated with `@tool`)
* Callable objects (functions) with proper type hints and a docstring
* `dict` that represents a built-in provider tools

It no longer accepts `ToolNode` instances.

<CodeGroup>
```python V1 (new)
from langchain.agents import create_agent

agent = create_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=[check_weather, search_web]
)
```
```python V0 (old)
from langgraph.prebuilt import create_react_agent, ToolNode

agent = create_react_agent(
    model="anthropic:claude-4-5-sonnet",
    tools=ToolNode([check_weather, search_web]) # [!code highlight]
)
```
</CodeGroup>

#### Handling tool errors

You can now configure the handling of tool errors with middleware implementing the `wrap_tool_call` method.

<CodeGroup>
```python V1 (new)
# Example coming soon
```
```python V0 (old)
# Example coming soon
```
</CodeGroup>

### Structured output

#### Node Changes

Structured output used to be generated in a separate node from the main agent. This is no longer the case.
We generate structured output in the main loop, reducing cost and latency.

#### Tool and provider strategies

In V1 we're announcing two new structured output strategies:

* `ToolStrategy` uses artificial tool calling to generate structured output
* `ProviderStrategy` uses provider-native structured output generation

<CodeGroup>
```python V1 (new)
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy, ProviderStrategy
from pydantic import BaseModel

class OutputSchema(BaseModel):
    summary: str
    sentiment: str

# Using ToolStrategy
agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=tools,
    # explicitly using tool strategy
    response_format=ToolStrategy(OutputSchema)  # [!code highlight]
)
```

```python V0 (old)
from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel

class OutputSchema(BaseModel):
    summary: str
    sentiment: str

agent = create_react_agent(
    model="openai:gpt-4o-mini",
    tools=tools,
    # using tool strategy by default with no option for provider strategy
    response_format=OutputSchema  # [!code highlight]
)

# OR

agent = create_react_agent(
    model="openai:gpt-4o-mini",
    tools=tools,
    # using a custom prompt to instruct the model to generate the output schema
    response_format=("please generate ...", OutputSchema)  # [!code highlight]
)
```
</CodeGroup>

#### Prompted output removed

**Prompted output** is no longer supported via the `response_format` argument. Compared to strategies
like artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable.

### Streaming node name rename

When streaming events from agents, the node name has changed from `"agent"` to `"model"` to better reflect the node's purpose.

{/* TODO: add diagrams */}

### Runtime context

When you invoke an agent, it's often the case that you want to pass two types of data:
* Dynamic state that changes throughout the conversation (ex, message history)
* Static context that doesn't change during the conversation (ex, user metadata)

In V1, we've added first class support for static context with the `context` parameter to `invoke` and `stream`.

<CodeGroup>
```python V1 (new)
from dataclasses import dataclass

from langchain.agents import create_agent

@dataclass
class Context:
    user_id: str
    session_id: str

agent = create_agent(
    model=model,
    tools=tools,
    context_schema=ContextSchema  # [!code highlight]
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "Hello"}]},
    context=Context(user_id="123", session_id="abc")  # [!code highlight]
)
```
```python V0 (old)
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(model, tools)

# Pass context via configurable
result = agent.invoke(
    {"messages": [{"role": "user", "content": "Hello"}]},
    config={  # [!code highlight]
        "configurable": {  # [!code highlight]
            "user_id": "123",  # [!code highlight]
            "session_id": "abc"  # [!code highlight]
        }  # [!code highlight]
    }  # [!code highlight]
)
```
</CodeGroup>

<Note>
    The old `config["configurable"]` pattern still works for backward compatibility, but using the new `context` parameter is recommended for new applications or applications migrating to V1.
</Note>

---

## General Changes

### Dropped Python 3.9 support

All LangChain packages now require **Python 3.10 or higher**. Python 3.9 reaches [end of life](https://devguide.python.org/versions/) in October 2025.

### Updated return type for chat models

The return type signature for chat model invocation has been fixed from `BaseMessage` to `AIMessage`. Custom chat models implementing `bind_tools` should update their return signature:

<CodeGroup>
```python V1 (new)
Runnable[LanguageModelInput, AIMessage]
```
```python V0 (old)
Runnable[LanguageModelInput, BaseMessage]
```
</CodeGroup>

### Default message format for OpenAI Responses API

When interacting with the Responses API, `langchain-openai` now defaults to storing response items in message `content`. To restore previous behavior, set the `LC_OUTPUT_VERSION` environment variable to `v0`, or specify `output_version="v0"` when instantiating `ChatOpenAI`.

```python
# enforce previous behavior with output_version flag
model = ChatOpenAI(model="gpt-4o-mini", output_version="v0")
```

### Default `max_tokens` in `langchain-anthropic`

The `max_tokens` parameter now defaults to higher values based on the model chosen, rather than the previous default of `1024`. If you relied on the old default, explicitly set `max_tokens=1024`.

### Legacy code moved to `langchain-classic`

Existing functionality outside the focus of standard interfaces and agents has been moved to the `langchain-classic` package. Install `langchain-classic` and update imports from `langchain` to `langchain_classic`.

<CodeGroup>
```python V1 (new)
from langchain_classic import SomeClassicFeature

# Use classic functionality
classic_feature = SomeClassicFeature()
```

```python V0 (old)
from langchain import SomeClassicFeature

# Old import path
classic_feature = SomeClassicFeature()
```
</CodeGroup>

**Installation**:
```bash
uv pip install langchain-classic
```

### Removal of deprecated objects

Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the [deprecation notices](https://python.langchain.com/docs/versions/migrating_chains) from previous versions for replacement APIs.

### `.text()` is now a property

Use of the `.text()` method on message objects should drop the parentheses:

<CodeGroup>
```python V1 (new)
# Property access
text = response.text

# deprecated method call
text = response.text()
```
```python V0 (old)
text = response.text()
```
</CodeGroup>

Existing usage patterns (i.e., `.text()`) will continue to function but now emit a warning.

---

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langchain-v1.mdx)
</Callout>
