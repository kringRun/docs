---
title: Runtime
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

## Overview

LangChain's `create_agent` runs on LangGraph's runtime under the hood.


LangGraph exposes a [Runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.Runtime) object with the following information:

1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation
2. **Store**: a [BaseStore](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) instance used for [long-term memory](/oss/python/langchain/long-term-memory)
3. **Stream writer**: an object used for streaming information via the `"custom"` stream mode

You can access the runtime information within [tools](#inside-tools), [prompt](#inside-prompt), and [pre and post model hooks](#inside-pre-and-post-model-hooks).

## Access

When creating an agent with `create_agent`, you can specify a `context_schema` to define the structure of the `context` stored in the agent [Runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.Runtime).



When invoking the agent, pass the `context` argument with the relevant configuration for the run:

```python
from dataclasses import dataclass

from langchain.agents import create_agent

@dataclass
class Context:
    user_name: str

agent = create_agent(
    model="openai:gpt-5-nano",
    tools=[...],
    context_schema=Context  # [!code highlight]
)

agent.invoke(
    {"messages": [{"role": "user", "content": "What's my name?"}]},
    context=Context(user_name="John Smith")  # [!code highlight]
)
```



### Inside tools

You can access the runtime information inside tools to:

* Access the context
* Read or write long-term memory
* Write to the [custom stream](/oss/python/langchain/streaming#custom-updates) (ex, tool progress / updates)

Use the [get_runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.get_runtime) function from `langgraph.runtime` to access the [Runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.Runtime) object inside a tool.

```python
from langchain.tools import tool
from langgraph.runtime import get_runtime  # [!code highlight]

@tool
def fetch_user_email_preferences() -> str:
    """Fetch the user's email preferences from the store."""
    runtime = get_runtime(Context)  # [!code highlight]
    user_id = runtime.context.user_id  # [!code highlight]

    preferences: str = "The user prefers you to write a brief and polite email."
    if runtime.store:  # [!code highlight]
        if memory := runtime.store.get(("users",), user_id):  # [!code highlight]
            preferences = memory.value["preferences"]

    return preferences
```



### Inside prompt

Use the [get_runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.get_runtime) function from `langgraph.runtime` to access the [Runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.Runtime) object inside a prompt function.

```python
from dataclasses import dataclass

from langchain.messages import AnyMessage
from langchain.agents import create_agent
from langgraph.runtime import get_runtime  # [!code highlight]

@dataclass
class Context:
    user_name: str

from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt
def dynamic_system_prompt(request: ModelRequest) -> str:
    user_name = request.runtime.context["user_name"]
    system_prompt = f"You are a helpful assistant. Address the user as {user_name}."
    return system_prompt

agent = create_agent(
    model="openai:gpt-5-nano",
    tools=[...],
    middleware=[dynamic_system_prompt],
    context_schema=Context
)

agent.invoke(
    {"messages": [{"role": "user", "content": "What's my name?"}]},
    context=Context(user_name="John Smith")
)
```



### Inside pre and post model hooks

To access the underlying graph runtime information in a pre or post model hook, you can:

1. Use the [get_runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.get_runtime) function from `langgraph.runtime` to access the [Runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.Runtime) object inside the hook
2. Inject the [Runtime](https://langchain-ai.github.io/langgraph/reference/runtime/#langgraph.runtime.Runtime) directly via the hook signature

This above options are purely preferential and not functionally different.

<Tabs>
	<Tab title="Using get_runtime">
		```python
		from langgraph.runtime import get_runtime  # [!code highlight]

		def pre_model_hook(state: State) -> State:
		    runtime = get_runtime(Context)  # [!code highlight]
            ...
		```
	</Tab>
	<Tab title="Injection">
		```python
		from langgraph.runtime import Runtime  # [!code highlight]

		def pre_model_hook(state: State, runtime: Runtime[Context]):  # [!code highlight]
		    ...
		```
	</Tab>
</Tabs>

---

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/runtime.mdx)
</Callout>
