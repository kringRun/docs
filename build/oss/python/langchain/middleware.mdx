---
title: Middleware
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

Middleware provides a way to more tightly control what happens inside the agent.

The core agent loop involves calling a `model`, letting it choose `tools` to execute, and then finishing when it calls no more tools.

Middleware provides control over what happens before and after those steps.

![Middleware flow diagram](/images/middleware_final.png)

## Overview

Middleware intercepts the agent execution flow at specific points, allowing you to:

- **Monitor** execution (logging, analytics, debugging)
- **Modify** requests and responses (prompts, tool selection, output formatting)
- **Control** flow (early termination, retries, fallbacks)
- **Enforce** policies (rate limits, guardrails, PII detection)

### Available Hooks

Build custom middleware by implementing any of these hooks on a subclass of the `AgentMiddleware` class:

| Hook | When it runs | Use cases |
|------|--------------|-----------|
| `before_agent` | Before calling the agent (once per invocation) | Load memory, validate input |
| `before_model` | Before each LLM call | Update prompts, trim messages |
| `wrap_model_call` | Around each LLM call | Retry logic, fallbacks, caching |
| `after_model` | After each LLM response | Validate output, apply guardrails |
| `wrap_tool_call` | Around each tool call | Retry, monitoring, modification |
| `after_agent` | After agent completes (once per invocation) | Save results, cleanup |



**Node-style hooks** (`before_agent`, `before_model`, `after_model`, `after_agent`) run sequentially and can:
- Return state updates to merge into agent state
- Return `{"jump_to": "end/model/tools"}` to jump to a different part of the graph
- Access and modify `state` and `runtime`

**Wrap-style hooks** (`wrap_model_call`, `wrap_tool_call`) intercept execution and can:
- Call the `handler` function zero, one, or multiple times
- Modify requests before calling the handler
- Transform responses before returning
- Implement retry logic, caching, or short-circuiting

### Using in an agent

You can use middleware in an agent by passing it to `create_agent`:

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
    model="openai:gpt-4o",
    tools=[...],
    middleware=[SummarizationMiddleware(), HumanInTheLoopMiddleware()],
)
```




## Built-in middleware

LangChain provides several built in middleware to use off-the-shelf

- [Summarization](#summarization)
- [Human-in-the-loop](#human-in-the-loop)
- [Anthropic prompt caching](#anthropic-prompt-caching)

### Summarization

The `SummarizationMiddleware` automatically manages conversation history by summarizing older messages when token limits are approached. This middleware monitors the total token count of messages and creates concise summaries to preserve context while staying within model limits.

**Key features:**

- Automatic token counting and threshold monitoring
- Intelligent message partitioning that preserves AI/Tool message pairs
- Customizable summary prompts and token limits

**Use Cases:**

- Long-running conversations that exceed token limits
- Multi-turn dialogues with extensive context

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="openai:gpt-4o",
    tools=[weather_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model="openai:gpt-4o-mini",
            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens
            messages_to_keep=20,  # Keep last 20 messages after summary
            summary_prompt="Custom prompt for summarization...",  # Optional
        ),
    ],
)
```




**Configuration options:**

- `model`: Language model to use for generating summaries (required)
- `max_tokens_before_summary`: Token threshold that triggers summarization
- `messages_to_keep`: Number of recent messages to preserve (default: 20)
- `token_counter`: Custom function for counting tokens (defaults to character-based approximation)
- `summary_prompt`: Custom prompt template for summary generation
- `summary_prefix`: Prefix added to system messages containing summaries (default: "## Previous conversation summary:")




The middleware ensures tool call integrity by:

1. Never splitting AI messages from their corresponding tool responses
2. Preserving the most recent messages for continuity
3. Including previous summaries in new summarization cycles

### Human-in-the-loop

The `HumanInTheLoopMiddleware` enables human oversight and intervention for tool calls made by the agents. Please
see [human-in-the-loop documentation](/oss/python/python/langchain/human-in-the-loop) for more details.

This middleware intercepts tool executions and allows human operators to approve, modify, reject, or manually respond to tool calls before they execute.

### Anthropic prompt caching

`AnthropicPromptCachingMiddleware` is a middleware that enables you to enable Anthropic's native prompt caching.

Prompt caching enables optimal API usage by allowing resuming from specific prefixes in your prompts.
This is particularly useful for tasks with repetitive prompts or prompts with redundant information.

<Info>
Learn more about Anthropic Prompt Caching (strategies, limitations, etc.) [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#cache-limitations).
</Info>

When using prompt caching, you'll likely want to use a checkpointer to store conversation
history across invocations.

```python
from langchain_anthropic import ChatAnthropic
from langchain.agents.middleware.prompt_caching import AnthropicPromptCachingMiddleware
from langchain.agents import create_agent

LONG_PROMPT = """
Please be a helpful assistant.

<Lots more context ...>
"""

agent = create_agent(
    model=ChatAnthropic(model="claude-sonnet-4-latest"),
    system_prompt=LONG_PROMPT,
    middleware=[AnthropicPromptCachingMiddleware(ttl="5m")],
)

# cache store
agent.invoke({"messages": [HumanMessage("Hi, my name is Bob")]})

# cache hit, system prompt is cached
agent.invoke({"messages": [HumanMessage("What's my name?")]})
```




## Custom middleware

### Creating middleware with hooks

Middleware for agents are subclasses of `AgentMiddleware`, which implement one or more of its hooks.

There are two styles of hooks:

1. **Node-style hooks** - Run sequentially at specific points
2. **Wrap-style hooks** - Intercept and control execution via handler callbacks

#### Node-style hooks

Node-style hooks run sequentially and can return state updates or trigger jumps:

- `before_agent`: runs before the agent starts
- `before_model`: runs before each model call
- `after_model`: runs after each model response
- `after_agent`: runs after the agent completes

**Example: Logging middleware**

```python
from langchain.agents.middleware import AgentMiddleware, AgentState
from langgraph.runtime import Runtime
from typing import Any

class LoggingMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"About to call model with {len(state['messages'])} messages")
        return None

    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"Model returned: {state['messages'][-1].content}")
        return None
```




**Example: Conversation length limit**

```python
from langchain.agents.middleware import AgentMiddleware, AgentState
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

class MessageLimitMiddleware(AgentMiddleware):
    def __init__(self, max_messages: int = 50):
        super().__init__()
        self.max_messages = max_messages

    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        if len(state["messages"]) == self.max_messages:
            return {
                "messages": [AIMessage("Conversation limit reached.")],
                "jump_to": "end"
            }
        return None
```




#### Wrap-style hooks (Interceptors)

Wrap-style hooks intercept execution and provide a handler callback. You control when (and if) the handler is called:

- `wrap_model_call`: wraps each model call
- `wrap_tool_call`: wraps each tool call

The handler can be called:
- **Zero times** - Short-circuit with cached/pre-computed result
- **Once** - Normal execution with request/response modification
- **Multiple times** - Retry logic or iterative improvement

**Example: Model retry middleware**

```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable

class RetryMiddleware(AgentMiddleware):
    def __init__(self, max_retries: int = 3):
        super().__init__()
        self.max_retries = max_retries

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        for attempt in range(self.max_retries):
            try:
                return handler(request)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                print(f"Retry {attempt + 1}/{self.max_retries} after error: {e}")
```




**Example: Dynamic model selection**

```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

class DynamicModelMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        # Use different model based on conversation length
        if len(request.messages) > 10:
            request.model = init_chat_model("openai:gpt-4o")
        else:
            request.model = init_chat_model("openai:gpt-4o-mini")

        return handler(request)
```




**Example: Tool call monitoring**

```python
from langchain.tools.tool_node import ToolCallRequest
from langchain.agents.middleware import AgentMiddleware
from langchain_core.messages import ToolMessage
from langgraph.types import Command
from typing import Callable

class ToolMonitoringMiddleware(AgentMiddleware):
    def wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        print(f"Executing tool: {request.tool_call['name']}")
        print(f"Arguments: {request.tool_call['args']}")

        try:
            result = handler(request)
            print(f"Tool completed successfully")
            return result
        except Exception as e:
            print(f"Tool failed: {e}")
            raise
```




### Custom state schema

Middleware can extend the agent's state with custom properties. Define a custom state type and set it as the `state_schema`:

```python
from langchain.agents.middleware import AgentState, AgentMiddleware
from typing_extensions import NotRequired
from typing import Any

class CustomState(AgentState):
    model_call_count: NotRequired[int]
    user_id: NotRequired[str]

class CallCounterMiddleware(AgentMiddleware[CustomState]):
    state_schema = CustomState

    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        # Access custom state properties
        count = state.get("model_call_count", 0)

        if count > 10:
            return {"jump_to": "end"}

        return None

    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        # Update custom state
        return {"model_call_count": state.get("model_call_count", 0) + 1}
```




```python
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[CallCounterMiddleware()],
    tools=[...],
)

# Invoke with custom state
result = agent.invoke({
    "messages": [HumanMessage("Hello")],
    "model_call_count": 0,
    "user_id": "user-123",
})
```








### Decorator-based middleware

For simple middleware that only needs a single hook, use decorator shortcuts:

```python
from langchain.agents.middleware import before_model, after_model, wrap_model_call
from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse
from langgraph.runtime import Runtime
from typing import Any, Callable

# Node-style decorator
@before_model
def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"About to call model with {len(state['messages'])} messages")
    return None

# Wrap-style decorator
@wrap_model_call
def retry_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    for attempt in range(3):
        try:
            return handler(request)
        except Exception:
            if attempt == 2:
                raise
    return handler(request)  # This line is unreachable but satisfies type checker

# Use decorators in agent
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[log_before_model, retry_model],
    tools=[...],
)
```

Available decorators:
- `@before_agent`
- `@before_model`
- `@after_model`
- `@after_agent`
- `@wrap_model_call`
- `@wrap_tool_call`
- `@dynamic_prompt` - Convenience for dynamic system prompts


### Combining and ordering middleware

When using multiple middleware, they execute in a specific order:

**Node-style hooks:**
- `before_agent`: Runs in order (first to last)
- `before_model`: Runs in order (first to last)
- `after_model`: Runs in **reverse** order (last to first)
- `after_agent`: Runs in **reverse** order (last to first)

**Wrap-style hooks:**
- `wrap_model_call`: Composes like nested function calls (first middleware wraps all subsequent ones)
- `wrap_tool_call`: Composes like nested function calls

**Example:**

```python
agent = create_agent(
    model="openai:gpt-4o",
    middleware=[middleware1, middleware2, middleware3],
    tools=[...],
)
```




Execution order:
1. `middleware1.before_agent()`
2. `middleware2.before_agent()`
3. `middleware3.before_agent()`
4. **Agent loop starts**
5. `middleware1.before_model()`
6. `middleware2.before_model()`
7. `middleware3.before_model()`
8. `middleware1.wrap_model_call()` → calls `middleware2.wrap_model_call()` → calls `middleware3.wrap_model_call()` → actual model call
9. `middleware3.after_model()`
10. `middleware2.after_model()`
11. `middleware1.after_model()`
12. **Agent loop ends**
13. `middleware3.after_agent()`
14. `middleware2.after_agent()`
15. `middleware1.after_agent()`

### Agent jumps

To exit early from middleware, return a dictionary with `jump_to`:

```python
class EarlyExitMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState, runtime) -> dict[str, Any] | None:
        # Check some condition
        if should_exit(state):
            return {
                "messages": [AIMessage("Exiting early due to condition.")],
                "jump_to": "end"
            }
        return None
```




Available jump targets:

- `"end"`: Jump to the end of the agent execution
- `"tools"`: Jump to the tools node
- `"model"`: Jump to the model node (or the first `before_model` hook)

**Important:** When jumping from `before_model` or `after_model`, jumping to `"model"` will cause all `before_model` middleware to run again.

To enable jumping, decorate your hook with `@hook_config(can_jump_to=[...])`:

```python
from langchain.agents.middleware import AgentMiddleware, hook_config
from typing import Any

class ConditionalMiddleware(AgentMiddleware):
    @hook_config(can_jump_to=["end", "tools"])
    def after_model(self, state: AgentState, runtime) -> dict[str, Any] | None:
        if some_condition(state):
            return {"jump_to": "end"}
        return None
```




### Best practices

1. **Keep middleware focused** - Each middleware should do one thing well
2. **Handle errors gracefully** - Don't let middleware errors crash the agent
3. **Use appropriate hook types**:
   - Node-style for sequential logic (logging, validation)
   - Wrap-style for control flow (retry, fallback, caching)
4. **Document state requirements** - Clearly document any custom state properties
5. **Test middleware independently** - Unit test middleware before integrating
6. **Consider execution order** - Place critical middleware first in the list
7. **Use built-in middleware when possible** - Don't reinvent the wheel
:::

## Examples

### Dynamically selecting tools

In many applications, you may have a large set of tools, but only a small subset is relevant for a specific request. To optimize performance and accuracy, it's best to **expose only the tools that are needed for each request**.

Doing so provides several benefits:

* **Shorter prompts** – reducing unnecessary complexity.
* **Improved accuracy** – the model chooses from fewer options.
* **Permission control** – can select tools based on user permissions.

Use middleware to dynamically select which tools are available at runtime based on context.

```python
from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest
from typing import Callable

class ToolSelectorMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        """Middleware to select relevant tools based on state/context."""
        # Select a small, relevant subset of tools based on state/context
        relevant_tools = select_relevant_tools(request.state, request.runtime)
        request.tools = relevant_tools
        return handler(request)

agent = create_agent(
    model="openai:gpt-4o",
    tools=all_tools,  # All available tools need to be registered upfront
    # Middleware can be used to select a smaller subset that's relevant for the given run.
    middleware=[ToolSelectorMiddleware()],
)
```




<Expandable title="Extended example: Select tools based on runtime context">

This example shows how to select between GitHub and GitLab tools based on the user's provider.

```python
from dataclasses import dataclass
from typing import Literal, Callable

from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain_core.tools import tool

@tool
def github_create_issue(repo: str, title: str) -> dict:
    """Create an issue in a GitHub repository."""
    return {"url": f"https://github.com/{repo}/issues/1", "title": title}

@tool
def gitlab_create_issue(project: str, title: str) -> dict:
    """Create an issue in a GitLab project."""
    return {"url": f"https://gitlab.com/{project}/-/issues/1", "title": title}

all_tools = [github_create_issue, gitlab_create_issue]

@dataclass
class Context:
    provider: Literal["github", "gitlab"]

class ToolSelectorMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        """Select tools based on the VCS provider."""
        provider = request.runtime.context.provider

        if provider == "gitlab":
            selected_tools = [t for t in request.tools if t.name == "gitlab_create_issue"]
        else:
            selected_tools = [t for t in request.tools if t.name == "github_create_issue"]

        request.tools = selected_tools
        return handler(request)

agent = create_agent(
    model="openai:gpt-4o",
    tools=all_tools,
    middleware=[ToolSelectorMiddleware()],
    context_schema=Context,
)

# Invoke with GitHub context
agent.invoke(
    {
        "messages": [{"role": "user", "content": "Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`"}]
    },
    context=Context(provider="github"),
)
```




**Key points:**

- Register all tools with the agent upfront
- Use middleware to select the relevant subset per request
- Define required context properties using `context_schema`
- Use context for configuration that doesn't change during execution
- Use state for values that change during the agent run
:::

</Expandable>

---

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware.mdx)
</Callout>
