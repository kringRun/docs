---
title: Model caches
---

[Caching LLM calls](/oss/javascript/langchain/models#caching) can be useful for testing, cost savings, and speed.

Below are some integrations that allow you to cache results of individual LLM calls using different caches with different strategies.

<Columns cols={3}>
  <Card
    title="Azure Cosmos DB NoSQL Semantic Cache"
    icon="link"
    href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql"
    arrow="true"
    cta="View guide"
  >
  </Card>
</Columns>

---

<CardGroup cols={2}>
  <Card title="View Source" icon="eye" href="https://github.com/langchain-ai/docs/blob/main/src/oss/javascript/integrations/llm_caching/index.mdx">
    See the source of this page on GitHub
  </Card>
  <Card title="Edit Source" icon="pen-to-square" href="https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llm_caching/index.mdx">
    Edit the source of this page on GitHub
  </Card>
</CardGroup>
